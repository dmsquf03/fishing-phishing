import pandas as pd
import re
import requests
import logging
from tqdm import tqdm
import ssl


######### 키워드 추출 ##############
# 파이어베이스에서 'keyword' 노드의 데이터를 불러옴
keyword_ref = db.reference('keyword')
keywordsData = keyword_ref.get()

# 데이터를 DataFrame으로 변환
keywords_list = []
for key, value in keywordsData.items():
    keywords_list.append(value)

keywordsDf = pd.DataFrame(keywords_list)

# 'Keywords'와 'return' 칼럼 추출
keywords = keywordsDf['Keywords']
category = keywordsDf['return']

category_num = []

def extract_keywords(text_tokens, keywords):
    i = 0
    extracted_words = []   # 문자 텍스트 내에 포함되어 있는 키워드를 포함하고 있는 단어 리스트
    key_tokens = []        # 문자 텍스트 내에 포함되어 있는 키워드 리스트
    key_tokens_index = []  # 문자 텍스트 내에 포함되어 있는 키워드가 keywords DataFrame에서 몇번째 인덱스인지
    for word in text_tokens:
        for i in range(len(keywords)):
            if keywords[i] in word:
                extracted_words.append(word)
                key_tokens.append(keywords[i])
                key_tokens_index.append(i)
    
    return key_tokens, key_tokens_index


######### 키워드의 카테고리 추출 #############
def find_category(category, key_tokens_index, category_num):
    for index in key_tokens_index:
        category_num.append(category[index])
    category_num = set(category_num)
    category_num = list(category_num)
    return category_num


########### url 추출 #############
def extract_urls_1(text_tokens):
    
    urls = []
    
    url_pattern = re.compile(r'\b(?:https?://|www\.)\S+\b')

    # 정규 표현식과 매치되는 모든 URL 추출
    for token in text_tokens:
        result = re.match(url_pattern, token)
        if result != None:
            urls.append(token)
    return urls

def extract_urls_2(token, urls2):
    try:
        response = requests.get('https://' + token, timeout=3, verify=False)
        if response.status_code == 200:
            urls2.append(token)
            return token
    except Exception as e:
        logging.error(f"HTTPS 연결 오류 - {token}: {e}")
        
    
    try:
        response = requests.get('http://' + token, timeout=3)
        if response.status_code == 200:
            urls2.append(token)
            return token
    except Exception as e:
        logging.error(f"HTTP 연결 오류 - {token}: {e}")


############ url+키워드 최종 함수 ##############
def url_keywords_extraction(keywords):
    
    # 문자 전체 텍스트 입력
    text = "덕진구민여러분 사랑합니다.새해 가족모두 행복하십시오 정동영올림 택배 하나 naver.com"
    text_tokens = list(text.split())
    
    # URL 추출
    urls1 = extract_urls_1(text_tokens)
    for token in text_tokens:
        urls2 = [] 
        extract_urls_2(token, urls2)
    
    # 추출된 url 리스트
    urls = urls1+urls2
    
    if len(urls) <= 0:
        return "url 없음."
    
    # 키워드 추출 및 키워드의 카테고리 찾기
    keywords, keywords_index = extract_keywords(text_tokens, keywords)
    
    # 키워드가 없을 경우
    if len(keywords) <= 0:
        return "키워드 없음. 머신러닝 바로 돌리기"
    
    url_dataset_num = find_category(category, keywords_index, category_num)
    return urls, url_dataset_num

############## 리턴 값에 따른 스미싱 판단 ##############
# 1. url 없음 -> 스미싱 아님 출력
# 2. 키워드 없음 -> ML 판단 -> 결과 출력
# 3. 키워드 있음 -> 키워드 카테고리 whitelist 대조 -> (whitelist에 해당 url 없으면) ML 판단 -> 결과 출력

############## whitelist 대조 ##############
# URL whitelist 검색 함수 / 입력 (카테고리 리스트, 검색할 url) / 반환 (카테고리, 해당 Url의 Name값)
def search_in_firebase(node_names, search_url):
    for node_name in node_names:
        # 해당 노드의 참조를 가져옴
        whitelist_ref = db.reference(f'whitelist/{node_name}')
        
        # 참조에서 전체 데이터를 가져옴
        whitelist_data = ref.get()

        # 데이터가 없으면 다음 노드로 계속
        if not data:
            continue
        
        # 데이터를 순회하며 검색 URL이 있는지 확인
        for key, value in whitelist_data.items():
            if 'url' in value and value['url'] == search_url:
                # 'Name' 값과 함께 반환, 'Name' 필드가 없을 경우 None 반환
                return node_name, value.get('Name')

    return None, None  # URL이 데이터베이스에 없음
